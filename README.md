# HyperOpt-Algorithms
Welcome to the HyperOpt-Algorithms repository, a unified collection of scripts designed for hyperparameter optimization using various state-of-the-art techniques such as random search, Bayesian optimization, Hyperband pruners, and evolutionary computation methods like differential evolution. The scripts cater to different machine learning models, aiming to simplify and expedite the process of choosing the most effective set of hyperparameters for your model.

## Contents
- Optuna script for hyperparameter tuning using Bayesian optimization, with Hyperband pruner, illustrated on an MLP model with the MNIST dataset.
- Further scripts implementing other optimization methods coming soon!

## Getting Started
1. Clone this repository to your local machine using `https://github.com/<Your Github Username>/HyperOpt-Algorithms.git`
2. Install all dependencies from the `requirements.txt` file using `pip install -r requirements.txt`
3. Run the scripts individually to optimize your model's hyperparameters.

## Contribution
Contributions, issues, and feature requests are welcome. Feel free to check the issues page if you want to contribute.

## License
This project is licensed under the terms of the MIT license.

## Acknowledgements
- PyTorch for the MLP model and MNIST dataset.
- Optuna for the hyperparameter tuning framework.

Please note that the above script is an example of using Optuna, a Python library for hyperparameter optimization, with PyTorch on the MNIST dataset. Stay tuned for more scripts that showcase different optimization techniques!
****
